{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field, BucketIterator, interleave_keys\n",
    "from torchtext.datasets import TranslationDataset\n",
    "from torchtext.data import Example\n",
    "from mosestokenizer import *\n",
    "import torch\n",
    "\n",
    "from typing import Tuple\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### torchtext #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = '<s>'    # Start symbol\n",
    "EOS = '</s>'   # End symbol\n",
    "PAD = '<pad>'  # padding symbol\n",
    "\n",
    "# ex) 'I am a boy.' -> ['I', 'am', 'a', 'boy']\n",
    "tok_en = MosesTokenizer('en')\n",
    "tok_fr = MosesTokenizer('fr')\n",
    "\n",
    "# Field: Tensor로 표현할 데이터의 타입, 처리 프로세스 등을 정의하는 객체\n",
    "src = Field(sequential=True,\n",
    "            use_vocab=True,\n",
    "            pad_token=PAD,\n",
    "            tokenize=tok_en,\n",
    "            lower=True,\n",
    "            batch_first=True) # if=True shape:[Batch, length] else shape=[length, Batch]\n",
    "\n",
    "tgt = Field(sequential=True,\n",
    "            use_vocab=True,\n",
    "            pad_token=PAD,\n",
    "            tokenize=tok_fr,\n",
    "            lower=True,\n",
    "            init_token=BOS,\n",
    "            eos_token=EOS,\n",
    "            batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_f = 'data/data'\n",
    "\n",
    "# parallel data 각각 (en, de) 을 src Field 와 tgt Field에 정의된 형태로 처리.\n",
    "parallel_dataset = TranslationDataset(path=prefix_f, exts=('.en', '.fr'), \n",
    "                                      fields=[('src', src), ('tgt', tgt)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.datasets.translation.TranslationDataset object at 0x7f3496a52eb0>\n",
      "dict_items([('src', ['you', 'were', 'in', 'a', 'coma', '.']), ('tgt', ['tu', 'étais', 'dans', 'le', 'coma', '.'])])\n"
     ]
    }
   ],
   "source": [
    "print(parallel_dataset) \n",
    "\n",
    "print(parallel_dataset.examples[22222].__dict__.items()) # src 및 tgt 에 대한 samples 를 포함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'were', 'in', 'a', 'coma', '.']\n"
     ]
    }
   ],
   "source": [
    "print(parallel_dataset.examples[22222].src) # src 출력 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tu', 'étais', 'dans', 'le', 'coma', '.']\n"
     ]
    }
   ],
   "source": [
    "print(parallel_dataset.examples[22222].tgt) # tgt 출력 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 사전 구축 ########\n",
    "# src, tgt 필드에 사전 구축\n",
    "src.build_vocab(parallel_dataset, max_size=15000)\n",
    "tgt.build_vocab(parallel_dataset, max_size=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['freqs', 'itos', 'unk_index', 'stoi', 'vectors'])\n",
      "\n",
      "     <unk> |   0\n",
      "     <pad> |   1\n",
      "         . |   2\n",
      "         i |   3\n",
      "       you |   4\n",
      "        to |   5\n",
      "       the |   6\n",
      "         ? |   7\n",
      "         a |   8\n",
      "   &apos;t |   9\n",
      "        is |  10\n",
      "        it |  11\n",
      "        he |  12\n",
      "      that |  13\n",
      "   &apos;s |  14\n",
      "        of |  15\n"
     ]
    }
   ],
   "source": [
    "# 사전 내용 \n",
    "print(src.vocab.__dict__.keys())\n",
    "print('')\n",
    "# stoi : string to index 의 약자\n",
    "for i, (k, v) in enumerate(src.vocab.stoi.items()):\n",
    "    print ('{:>10s} | {:>3d}'.format(k, v))\n",
    "    if i == 15 : break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = parallel_dataset.split(split_ratio=0.95) # 0.95 = train / 0.05 = valid 데이터로 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch iterator 생성.\n",
    "# iterator 를 반복하며 batch (src, tgt) 가 생성 됨.\n",
    "BATCH_SIZE = 3\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator = BucketIterator.splits((train, valid), batch_size=BATCH_SIZE,\n",
    "                                                    sort_key=lambda x: interleave_keys(len(x.src), len(x.tgt)),\n",
    "                                                    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator 는 Batch 객체 (Tensor) 를 출력해주며, \n",
    "# Batch.src / Batch.tgt 로 parallel data각각에 대해 접근가능.\n",
    "\n",
    "# 예시.\n",
    "Batch = next(iter(train_iterator)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  22,  166,  128,    9,   58,  108, 4824,    2,    1,    1,    1],\n",
       "        [   4,   70, 2619,   23,  505,    9,    4,    7,    1,    1,    1],\n",
       "        [   3,   34, 2067,    8,  204,   15,  373,   19,    6,  683,    2]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# src 에 저장된 데이터 출력\n",
    "# Field에 정의된 형식으로 데이터 전처리 (indexing 포함.)\n",
    "# 가장 긴 문장을 기준으로, 그 보다 짧은 문장은 Padding idx(=1) 을 부여.\n",
    "Batch.src "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,    70,   337,    15,  6684,     9,   111,     6,   679,     4,\n",
       "             3,     1,     1,     1],\n",
       "        [    2,    10,   355, 10640,    25,    23,    11,     7,    22,     9,\n",
       "             8,     3,     1,     1],\n",
       "        [    2,     5,   852,   524,    14,    24,   250,    14,    52,    14,\n",
       "            20,   744,     4,     3]], device='cuda:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Field에 정의된 형식으로 데이터 전처리 (indexing + bos + eos + pad 토큰 처리 됨.)\n",
    "Batch.tgt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Network 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Encoder 정의.\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, src_ntoken: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim # Encoder hidden size\n",
    "        self.dropout = dropout       # Dropout prob. \n",
    "        self.src_ntoken = src_ntoken # Vocab size\n",
    "\n",
    "        # Practice\n",
    "        self.embedding = nn.Embedding(?, ?, \n",
    "                                      padding_idx=src.vocab.stoi['<pad>'])\n",
    "        \n",
    "        # Practice\n",
    "        self.rnn = nn.GRU(?, ?, bidirectional = True, \n",
    "                          batch_first=True) # batch_first = [B, L, dim]\n",
    "        \n",
    "        \n",
    "        # bidirectional hidden을 하나의 hidden size로 mapping해주기 위한 Linear\n",
    "        self.fc = nn.Linear(hidden_dim * 2, hidden_dim) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src = (Batch, Length) Tensor\n",
    "        embedded = self.dropout(self.embedding(src)) # shape = (Batch, Length, hidden_dim)\n",
    "\n",
    "        # outputs: [B, L, D*2], hidden: [2, B, D] -> [1, B, D] + [1, B, D]\n",
    "        # Note: Bidirection=False 인 경우:  outputs: [B, L, D], hidden: [1, B, D]\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "\n",
    "        last_hidden = self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)) # bidirection Dim(x2)을 projection --> [B, D]\n",
    "        hidden = torch.tanh(last_hidden).unsqueeze(0) # last bidirectional hidden (=Decoder init hidden) --> [1, B, D]\n",
    "\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Attention 모듈 정의 ###\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 enc_hid_dim: int,\n",
    "                 dec_hid_dim: int,\n",
    "                 attn_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "\n",
    "        attn_in = (enc_hid_dim * 2) + dec_hid_dim # bidirectional hidden + dec_hidden\n",
    "        self.linear = nn.Linear(attn_in, attn_dim)\n",
    "        self.merge = nn.Linear(attn_dim, 1)\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        src_len = encoder_outputs.shape[1] \n",
    "        decoder_hidden = decoder_hidden.permute(1, 0, 2) # [1, Batch, Dim] -> [Batch, 1, Dim]\n",
    "        repeated_decoder_hidden = decoder_hidden.repeat(1, src_len, 1) # [Batch, 1, Dim] -> [Batch, srcLen, Dim]\n",
    "\n",
    "        \n",
    "        # enc의 각 step의 hidden + decoder의 hidden 의 결과값 # [B, src_len, EncDim + DecDim] --> [B, src_len, DecDim]\n",
    "        # tanh(W*h_dec  + U*h_enc) 수식 부분. \n",
    "        energy = # Practice\n",
    "        \n",
    "        # [B, src_len] 각 src 단어에 대한 점수 -> V^T tanh(W*h_dec  + U*h_enc) 부분\n",
    "        score = self.merge(energy).squeeze(-1) \n",
    "        \n",
    "        # softmax를 통해 확률분포값으로 변환\n",
    "        normalized_score = F.softmax(score, dim=1)  \n",
    "        return  normalized_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder 모듈\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, dec_ntoken: int, dropout: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim # Decoder RNN의 previous hidden\n",
    "        self.dropout = dropout\n",
    "        self.attention = Attention(enc_hid_dim=hidden_dim, \n",
    "                                   dec_hid_dim=hidden_dim, \n",
    "                                   attn_dim=hidden_dim) # attention layer\n",
    "        \n",
    "        self.dec_ntoken = dec_ntoken # tgt vocab_size\n",
    "\n",
    "        self.embedding = nn.Embedding(dec_ntoken, hidden_dim, \n",
    "                                      padding_idx=tgt.vocab.stoi['<pad>'])\n",
    "        \n",
    "        self.rnn = nn.GRU(hidden_dim, hidden_dim, batch_first=True) # bidirectinal=False 임.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.inner_linear = nn.Linear(self.hidden_dim*3, self.hidden_dim) # RNN 입력 크기\n",
    "        self.out_linear = nn.Linear(self.hidden_dim, dec_ntoken) # Vocab 크기로 linear projection\n",
    "        self.sm = nn.LogSoftmax(dim=-1) # 확률 분포 값.\n",
    "\n",
    "    def _context_rep(self, dec_out, enc_outs):\n",
    "        scores = self.attention(dec_out, enc_outs) # score = [B, src_len]\n",
    "        scores = scores.unsqueeze(1) # [B, 1, src_len] -> weight value (softmax)\n",
    "\n",
    "        # scores: (batch, 1, src_len),  ecn_outs: (Batch, src_len, dim)\n",
    "        context_vector = torch.bmm(scores, enc_outs) # weighted average -> (batch, 1, EncDim): encoder의 각 hidden의 weighted sum\n",
    "        return context_vector\n",
    "\n",
    "    def forward(self, input, decoder_hidden, encoder_outputs):\n",
    "        dec_outs = []\n",
    "        embedded = self.dropout(self.embedding(input)) # (Batch, length, Dim)\n",
    "        \n",
    "        # (Batch, 1, dim)  (batch, 1, dim) , ....,\n",
    "        for emb_t in embedded.split(1, dim=1): # Batch 별 각 단어 (=각 time step) 에 대한 embedding 출력 \n",
    "            context = self._context_rep(decoder_hidden, encoder_outputs) # Context vector: [Batch, 1, dim]\n",
    "            rnn_input = self.inner_linear(torch.cat([emb_t, context], dim=2)) # RNN 입력 차원으로 조정: [Batch, 1, dim]\n",
    "            rnn_out, decoder_hidden = self.rnn(rnn_input, decoder_hidden)\n",
    "            dec_out = self.out_linear(rnn_out) # Vocab 크기로 조정: [Batch, dim] -> [Batch, vocab]\n",
    "            dec_outs += [self.sm(dec_out)] # 현재 time step 결과 저장\n",
    "\n",
    "        if len(dec_outs) > 1:\n",
    "            dec_outs = dec_outs[:-1] # trg = trg[:-1] # <E> 는 Decoder 입력으로 고려하지 않음.\n",
    "            dec_outs = torch.cat(dec_outs, dim=1) # convert list into tensor : [B, L, vocab]\n",
    "\n",
    "        else: # step-wise 로 decoding 하는 경우,\n",
    "            dec_outs = dec_outs[0] # [B=1, L=1, vocab]\n",
    "\n",
    "        return dec_outs, decoder_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Seq-to-Seq 모델 정의 ###\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder: nn.Module,\n",
    "                 decoder: nn.Module,\n",
    "                 device: torch.device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        encoder_outputs, hidden = self.encoder(src) # encoder_outputs = (Batch, length, Dim * 2) , hidden = (Batch, Dim)\n",
    "        dec_out, _ = self.decoder(trg, hidden, encoder_outputs)\n",
    "        return dec_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(src.vocab)  # src 사전 크기\n",
    "OUTPUT_DIM = len(tgt.vocab) # tgt 사전 크기\n",
    "HID_DIM = 128 # rnn, embedding, 등. 모든 hidden 크기를 해당 값으로 통일함. (실습의 용이성을 위함.)\n",
    "D_OUT = 0.1 # Dropout  확률\n",
    "BATCH_SIZE = 60\n",
    "\n",
    "train_iterator, valid_iterator = BucketIterator.splits((train, valid), batch_size=BATCH_SIZE,\n",
    "                                                    sort_key=lambda x: interleave_keys(len(x.src), len(x.tgt)),\n",
    "                                                    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 및 디코더 생성\n",
    "# Seq2Seq 모델 생성\n",
    "encoder = Encoder(HID_DIM, INPUT_DIM, D_OUT)\n",
    "decoder = Decoder(HID_DIM, OUTPUT_DIM, D_OUT)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "\n",
    "model.apply(init_weights) # 모델 파라미터 초기화\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005) # Optimizer 설정\n",
    "criterion = nn.NLLLoss(ignore_index=tgt.vocab.stoi['<pad>'], reduction='mean') # LOSS 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(13296, 128, padding_idx=1)\n",
      "    (rnn): GRU(128, 128, batch_first=True, bidirectional=True)\n",
      "    (fc): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (linear): Linear(in_features=384, out_features=128, bias=True)\n",
      "      (merge): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "    (embedding): Embedding(15004, 128, padding_idx=1)\n",
      "    (rnn): GRU(128, 128, batch_first=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (inner_linear): Linear(in_features=384, out_features=128, bias=True)\n",
      "    (out_linear): Linear(in_features=128, out_features=15004, bias=True)\n",
      "    (sm): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "The model has 5,986,717 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# 모델 정보 및 파라미터 수 출력\n",
    "def count_parameters(model: nn.Module):\n",
    "    print(model)\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모델 학습 함수 ###\n",
    "def train(model, iterator, optimize, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src\n",
    "        tgt = batch.tgt\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, tgt) # [batch, length, vocab_size]\n",
    "        output = output.view(-1, output.size(-1)) # flatten --> (batch * length, vocab_size)\n",
    "\n",
    "        tgt = tgt.unsqueeze(-1)[:,1:,:].squeeze(-1).contiguous() # 정답에는 <S>가 포함되지 않음으로, 이를 삭제\n",
    "        tgt = tgt.view(-1) # flatten = (batch * length)\n",
    "\n",
    "        loss = criterion(output, tgt) # tgt 이 내부적으로 one_hot으로 변환됨 --> (batch * length, vocab_size)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if(((i+1) % int(len(iterator)*0.2)) == 0):\n",
    "            num_complete = batch.batch_size * (i+1)\n",
    "            total_size = batch.batch_size * int(len(iterator))\n",
    "            ratio = num_complete/total_size * 100\n",
    "            print('| Current Epoch:  {:>4d} / {:<5d} ({:2d}%) | Train Loss: {:3.3f}'.\n",
    "                  format(num_complete, batch.batch_size * int(len(iterator)), round(ratio), loss.item())\n",
    "                  )\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 모델 평가 함수 ###\n",
    "def evaluate(model: nn.Module, iterator: BucketIterator,\n",
    "             criterion: nn.Module):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            tgt = batch.tgt\n",
    "\n",
    "            output = model(src, tgt)\n",
    "            output = output.view(-1, output.size(-1)) # flatten (batch * length, vocab_size)\n",
    "\n",
    "            tgt = tgt.unsqueeze(-1)[:,1:,:].squeeze(-1).contiguous() # remove <S> placed at first from targets\n",
    "            tgt = tgt.view(-1) # flatten target with shape = (batch * length)\n",
    "            loss = criterion(output, tgt)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 시간 카운트를 위한 함수 #\n",
    "def epoch_time(start_time: int, end_time: int):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Current Epoch:  25800 / 129060 (20%) | Train Loss: 5.584\n",
      "| Current Epoch:  51600 / 129060 (40%) | Train Loss: 5.738\n",
      "| Current Epoch:  77400 / 129060 (60%) | Train Loss: 5.759\n",
      "| Current Epoch:  103200 / 129060 (80%) | Train Loss: 5.859\n",
      "| Current Epoch:  129000 / 129060 (100%) | Train Loss: 5.842\n",
      "=================================================================\n",
      "Epoch: 01 | Time: 2m 55s\n",
      "\tTrain Loss: 5.822 | Train PPL: 337.601\n",
      "\t Val. Loss: 5.707 |  Val. PPL: 300.819\n",
      "=================================================================\n",
      "| Current Epoch:  25800 / 129060 (20%) | Train Loss: 5.733\n",
      "| Current Epoch:  51600 / 129060 (40%) | Train Loss: 5.603\n",
      "| Current Epoch:  77400 / 129060 (60%) | Train Loss: 5.648\n",
      "| Current Epoch:  103200 / 129060 (80%) | Train Loss: 5.600\n",
      "| Current Epoch:  129000 / 129060 (100%) | Train Loss: 5.691\n",
      "=================================================================\n",
      "Epoch: 02 | Time: 2m 50s\n",
      "\tTrain Loss: 5.744 | Train PPL: 312.214\n",
      "\t Val. Loss: 5.702 |  Val. PPL: 299.359\n",
      "=================================================================\n",
      "| Current Epoch:  25800 / 129060 (20%) | Train Loss: 5.683\n",
      "| Current Epoch:  51600 / 129060 (40%) | Train Loss: 5.818\n",
      "| Current Epoch:  77400 / 129060 (60%) | Train Loss: 5.763\n",
      "| Current Epoch:  103200 / 129060 (80%) | Train Loss: 5.729\n",
      "| Current Epoch:  129000 / 129060 (100%) | Train Loss: 5.704\n",
      "=================================================================\n",
      "Epoch: 03 | Time: 2m 55s\n",
      "\tTrain Loss: 5.743 | Train PPL: 312.082\n",
      "\t Val. Loss: 5.702 |  Val. PPL: 299.344\n",
      "=================================================================\n",
      "| Current Epoch:  25800 / 129060 (20%) | Train Loss: 5.704\n",
      "| Current Epoch:  51600 / 129060 (40%) | Train Loss: 5.781\n",
      "| Current Epoch:  77400 / 129060 (60%) | Train Loss: 5.932\n",
      "| Current Epoch:  103200 / 129060 (80%) | Train Loss: 5.841\n",
      "| Current Epoch:  129000 / 129060 (100%) | Train Loss: 5.686\n",
      "=================================================================\n",
      "Epoch: 04 | Time: 2m 52s\n",
      "\tTrain Loss: 5.743 | Train PPL: 312.097\n",
      "\t Val. Loss: 5.703 |  Val. PPL: 299.670\n",
      "=================================================================\n",
      "| Current Epoch:  25800 / 129060 (20%) | Train Loss: 5.527\n",
      "| Current Epoch:  51600 / 129060 (40%) | Train Loss: 5.400\n",
      "| Current Epoch:  77400 / 129060 (60%) | Train Loss: 5.144\n",
      "| Current Epoch:  103200 / 129060 (80%) | Train Loss: 5.191\n",
      "| Current Epoch:  129000 / 129060 (100%) | Train Loss: 5.049\n",
      "=================================================================\n",
      "Epoch: 05 | Time: 2m 50s\n",
      "\tTrain Loss: 5.342 | Train PPL: 208.905\n",
      "\t Val. Loss: 5.084 |  Val. PPL: 161.435\n",
      "=================================================================\n",
      "| Current Epoch:  25800 / 129060 (20%) | Train Loss: 4.978\n",
      "| Current Epoch:  51600 / 129060 (40%) | Train Loss: 4.928\n",
      "| Current Epoch:  77400 / 129060 (60%) | Train Loss: 4.978\n",
      "| Current Epoch:  103200 / 129060 (80%) | Train Loss: 4.889\n",
      "| Current Epoch:  129000 / 129060 (100%) | Train Loss: 4.883\n",
      "=================================================================\n",
      "Epoch: 06 | Time: 2m 53s\n",
      "\tTrain Loss: 5.004 | Train PPL: 148.982\n",
      "\t Val. Loss: 4.839 |  Val. PPL: 126.331\n",
      "=================================================================\n",
      "| Current Epoch:  25800 / 129060 (20%) | Train Loss: 4.779\n",
      "| Current Epoch:  51600 / 129060 (40%) | Train Loss: 4.555\n",
      "| Current Epoch:  77400 / 129060 (60%) | Train Loss: 4.122\n",
      "| Current Epoch:  103200 / 129060 (80%) | Train Loss: 4.014\n",
      "| Current Epoch:  129000 / 129060 (100%) | Train Loss: 4.102\n",
      "=================================================================\n",
      "Epoch: 07 | Time: 2m 59s\n",
      "\tTrain Loss: 4.402 | Train PPL:  81.630\n",
      "\t Val. Loss: 3.882 |  Val. PPL:  48.502\n",
      "=================================================================\n",
      "| Current Epoch:  25800 / 129060 (20%) | Train Loss: 3.586\n",
      "| Current Epoch:  51600 / 129060 (40%) | Train Loss: 3.706\n",
      "| Current Epoch:  77400 / 129060 (60%) | Train Loss: 3.653\n",
      "| Current Epoch:  103200 / 129060 (80%) | Train Loss: 3.316\n",
      "| Current Epoch:  129000 / 129060 (100%) | Train Loss: 3.416\n",
      "=================================================================\n",
      "Epoch: 08 | Time: 2m 58s\n",
      "\tTrain Loss: 3.671 | Train PPL:  39.272\n",
      "\t Val. Loss: 3.393 |  Val. PPL:  29.752\n",
      "=================================================================\n",
      "| Current Epoch:  25800 / 129060 (20%) | Train Loss: 3.208\n",
      "| Current Epoch:  51600 / 129060 (40%) | Train Loss: 3.479\n",
      "| Current Epoch:  77400 / 129060 (60%) | Train Loss: 3.287\n",
      "| Current Epoch:  103200 / 129060 (80%) | Train Loss: 3.108\n",
      "| Current Epoch:  129000 / 129060 (100%) | Train Loss: 3.112\n",
      "=================================================================\n",
      "Epoch: 09 | Time: 2m 52s\n",
      "\tTrain Loss: 3.255 | Train PPL:  25.930\n",
      "\t Val. Loss: 3.048 |  Val. PPL:  21.071\n",
      "=================================================================\n",
      "| Current Epoch:  25800 / 129060 (20%) | Train Loss: 2.901\n",
      "| Current Epoch:  51600 / 129060 (40%) | Train Loss: 2.925\n",
      "| Current Epoch:  77400 / 129060 (60%) | Train Loss: 2.996\n",
      "| Current Epoch:  103200 / 129060 (80%) | Train Loss: 2.990\n",
      "| Current Epoch:  129000 / 129060 (100%) | Train Loss: 2.799\n",
      "=================================================================\n",
      "Epoch: 10 | Time: 2m 50s\n",
      "\tTrain Loss: 2.919 | Train PPL:  18.527\n",
      "\t Val. Loss: 2.790 |  Val. PPL:  16.277\n",
      "=================================================================\n",
      "| Current Epoch:  25800 / 129060 (20%) | Train Loss: 2.797\n",
      "| Current Epoch:  51600 / 129060 (40%) | Train Loss: 2.696\n",
      "| Current Epoch:  77400 / 129060 (60%) | Train Loss: 2.436\n",
      "| Current Epoch:  103200 / 129060 (80%) | Train Loss: 2.641\n",
      "| Current Epoch:  129000 / 129060 (100%) | Train Loss: 2.380\n",
      "=================================================================\n",
      "Epoch: 11 | Time: 2m 52s\n",
      "\tTrain Loss: 2.645 | Train PPL:  14.090\n",
      "\t Val. Loss: 2.570 |  Val. PPL:  13.062\n",
      "=================================================================\n",
      "| Current Epoch:  25800 / 129060 (20%) | Train Loss: 2.704\n",
      "| Current Epoch:  51600 / 129060 (40%) | Train Loss: 2.730\n",
      "| Current Epoch:  77400 / 129060 (60%) | Train Loss: 2.497\n",
      "| Current Epoch:  103200 / 129060 (80%) | Train Loss: 2.448\n",
      "| Current Epoch:  129000 / 129060 (100%) | Train Loss: 2.338\n",
      "=================================================================\n",
      "Epoch: 12 | Time: 2m 53s\n",
      "\tTrain Loss: 2.416 | Train PPL:  11.202\n",
      "\t Val. Loss: 2.371 |  Val. PPL:  10.707\n",
      "=================================================================\n",
      "model saving..\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 12 # 최대 epoch 크기\n",
    "CLIP = 0.2 # weight cliping \n",
    "isTrain = True # True 인 경우 아래 학습 코드 실행, False인 경우 저장된 model 로드만 수행.\n",
    "\n",
    "if isTrain:\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "        valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        print('='*65)\n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        print('='*65)\n",
    "\n",
    "    with open('NMT.pt', 'wb') as f:\n",
    "        print(\"model saving..\")\n",
    "        torch.save(model, f)\n",
    "\n",
    "else:\n",
    "    with open('NMT.pt', 'rb') as f:\n",
    "        model = torch.load(f).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decoding(model: nn.Module, input, fields, maxLen=20):\n",
    "    src_field = [('src', fields[0])]\n",
    "    tgt_field = fields[1]\n",
    "\n",
    "    ex = Example.fromlist([input], src_field)\n",
    "    src_tensor = src.numericalize([ex.src], device)\n",
    "    tgt_tensor = torch.tensor([[tgt.vocab.stoi['<s>']]], device=device)\n",
    "    model.eval()\n",
    "\n",
    "    dec_result = []\n",
    "    with torch.no_grad():\n",
    "        enc_out, hidden = model.encoder(src_tensor)\n",
    "        for i in range(maxLen):\n",
    "            dec_step, hidden = model.decoder(?, ?) # Practice\n",
    "            _, top_idx = torch.topk(dec_step, 1)   # greedy output\n",
    "            \n",
    "            # if top_idx == </S> : Stop translating\n",
    "            # else: Keep translating\n",
    "            \n",
    "    dec_result = [tgt_field.vocab.itos[w] for w in dec_result]\n",
    "    return dec_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a english sentence:  I went to the theater with her last night.\n",
      ">  I went to the theater with her last night.\n",
      "<  je suis allé à la fête avec son frère le jour.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Greedy decoding 수행\n",
    "\n",
    "input_sent = input('Enter a english sentence:  ')\n",
    "output = greedy_decoding(model, input_sent, fields=(src, tgt))\n",
    "output = MosesDetokenizer('fr')(output)\n",
    "print('> ', input_sent)\n",
    "print('< ', output)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
